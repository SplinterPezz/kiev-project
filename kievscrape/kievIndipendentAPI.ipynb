{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests\n",
    "#pip install bs4\n",
    "#pip install spacy\n",
    "#pip install pymongo\n",
    "#pip install spacy download https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0.tar.gz\n",
    "#pip instal gensim\n",
    "#pip install word2number\n",
    "#pip install inflect\n",
    "#pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from word2number import w2n\n",
    "import inflect\n",
    "\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "mainUrl = \"https://kyivindependent.com/wp-json/wp/v2/posts/?orderby=date&per_page=100&page=\"\n",
    "\n",
    "payload={}\n",
    "headers = {\n",
    "  'authority': 'kyivindependent.com',\n",
    "  'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "  'accept-language': 'it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6',\n",
    "  'cache-control': 'no-cache',\n",
    "  'cookie': '_hjSessionUser_2741431=eyJpZCI6IjhmODQ0YjgyLWRmMzktNTg4OC1hMzAxLTkwZTUwNzlhYzQ5NyIsImNyZWF0ZWQiOjE2NDU3Mjk3MTcwNzMsImV4aXN0aW5nIjp0cnVlfQ==; cf_clearance=0B7qGXQrTfbNX6JsyaQag6ZGakcuBPEV8JgBYNQCvyA-1646492515-0-150; cookie_notice_accepted=true; _gid=GA1.2.163750293.1648756295; _hjSession_2741431=eyJpZCI6IjQ2ZDQwNTNkLTgxZDQtNDE0Ny04ZTVhLTkwZjM0MDFiMWFlNyIsImNyZWF0ZWQiOjE2NDg3NTYyOTUxNTIsImluU2FtcGxlIjpmYWxzZX0=; _hjAbsoluteSessionInProgress=0; _hjIncludedInSessionSample=0; _ga_4WHBG5CRKW=GS1.1.1648756294.17.1.1648759737.0; _ga=GA1.2.1064471579.1645729717; mf_43294eee-4fef-49b5-83b6-fe216cdec496=|.12563616317.1648759737756|1648083465574||0|||0|0|18.86952',\n",
    "  'pragma': 'no-cache',\n",
    "  'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"100\", \"Google Chrome\";v=\"100\"',\n",
    "  'sec-ch-ua-mobile': '?0',\n",
    "  'sec-ch-ua-platform': '\"Windows\"',\n",
    "  'sec-fetch-dest': 'document',\n",
    "  'sec-fetch-mode': 'navigate',\n",
    "  'sec-fetch-site': 'none',\n",
    "  'sec-fetch-user': '?1',\n",
    "  'upgrade-insecure-requests': '1',\n",
    "  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36'\n",
    "}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "inflectEngine = inflect.engine()\n",
    "spacy.load('en_core_web_sm')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "end_parsing_string = \"The page number requested is larger than the number of pages available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_article_model(title, date, main, tag, custom_tag, deaths):\n",
    "    return {\n",
    "        'title': title,\n",
    "        'date': date,\n",
    "        'main': main,\n",
    "        'tag': tag,\n",
    "        'custom_tag': custom_tag,\n",
    "        'deaths': deaths\n",
    "    }\n",
    "\n",
    "def get_text_tokenized(text):\n",
    "    token_object = prepare_text_for_lda(text)\n",
    "    tokens = choose_best_lemma(token_object)\n",
    "    return remove_tag_duplicates(tokens)\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens2 = [get_lemma2(token) for token in tokens]\n",
    "    return {\n",
    "        \"first_lemma\": tokens,\n",
    "        \"second_lemma\": tokens2\n",
    "    }\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    return word if lemma is None else lemma\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def best_lemma(lemma1, lemma2):\n",
    "    return lemma2 if len(lemma1) > len(lemma2) else lemma1\n",
    "\n",
    "def choose_best_lemma(token_object):\n",
    "    if token_object['first_lemma'] is None and token_object['second_lemma'] is None:\n",
    "        return []\n",
    "    if token_object['first_lemma'] is None:\n",
    "        return token_object['second_lemma']\n",
    "    if token_object['second_lemma'] is None:\n",
    "        return token_object['first_lemma']\n",
    "    return best_lemma(token_object['first_lemma'], token_object['second_lemma'])\n",
    "\n",
    "def remove_tag_duplicates(tags):\n",
    "    tags_cleaned = []\n",
    "    for tag in tags:\n",
    "        if tag not in tags_cleaned:\n",
    "            tags_cleaned.append(tag)\n",
    "    return tags_cleaned\n",
    "\n",
    "def convert_numbers_in_text(text):\n",
    "    number_word_list = numbers_word_list(inflect_engine)\n",
    "    for number in number_word_list:\n",
    "        if number in text:\n",
    "            text = text.replace(number, str(w2n.word_to_num(number)))\n",
    "    return text\n",
    "\n",
    "def numbers_word_list(inflect_engine):\n",
    "    numbers_list = []\n",
    "    for i in range(100):\n",
    "        number_to_add = inflect_engine.number_to_words(i)\n",
    "        numbers_list.append(number_to_add)\n",
    "        if '-' in number_to_add:\n",
    "            numbers_list.append(number_to_add.replace('-', ''))\n",
    "    return numbers_list\n",
    "\n",
    "def search_death(article_body):\n",
    "    values = []\n",
    "    for synonym in synonims_deaths:\n",
    "        value = search_death_regex(synonym, article_body)\n",
    "\n",
    "        if len(value) > 0:\n",
    "            single_value = []\n",
    "            for s_value in value:\n",
    "                if s_value is not None and len(s_value) > 0:\n",
    "                    for any_val in s_value:\n",
    "                        if len(any_val) > 0:\n",
    "                            single_value.append(any_val)\n",
    "\n",
    "            for val in single_value:\n",
    "                if val not in values:\n",
    "                    values.append(val)\n",
    "    values_int = [int(v.replace(',', '')) for v in values]\n",
    "    return values_int\n",
    "\n",
    "def search_death_regex(synonym, text):\n",
    "    expression = r\"(?i)(?:\\b\" + synonym + r\"\\D{0,20})([0-9][0-9,]*)[^.,]|([0-9][0-9,]*)[^.,](?:\\D{0,20}\" + synonym + \")\"\n",
    "    values = re.findall(expression, text)\n",
    "    return values\n",
    "\n",
    "def get_content_text(article_json):\n",
    "    soup_text = BeautifulSoup(article_json['content']['rendered'], 'html5lib')\n",
    "    return soup_text.text.replace('\\n', '')\n",
    "\n",
    "def get_death_value(body_article):\n",
    "    article_body_converted = convert_numbers_in_text(body_article)\n",
    "    death_value = search_death(article_body_converted)\n",
    "    return death_value\n",
    "\n",
    "def date_str_to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def get_tag_from_slug(article_json):\n",
    "    slug = article_json['slug']\n",
    "    slug_update = slug.replace('-', ' ')\n",
    "    tags = get_text_tokenized(slug_update)\n",
    "    return tags\n",
    "\n",
    "def convert_json_page_to_articles_list(page_json, articles_list):\n",
    "    for article_json in page_json:\n",
    "        body_article = get_content_text(article_json)\n",
    "        custom_tags = get_text_tokenized(body_article)\n",
    "        death_value = get_death_value(body_article)\n",
    "        date = date_str_to_datetime(article_json['date'])\n",
    "        title = article_json['title']['rendered']\n",
    "        tags = get_tag_from_slug(article_json)\n",
    "        new_article = new_article_model(title, date, body_article, tags, custom_tags, death_value)\n",
    "        articles_list.append(new_article)\n",
    "    return articles_list\n",
    "\n",
    "def remove_duplicates(article_list):\n",
    "    new_list = []\n",
    "    article_titles = set()\n",
    "    duplicates_count = 0\n",
    "    for article in article_list:\n",
    "        if article['title'] not in article_titles:\n",
    "            article_titles.add(article['title'])\n",
    "            new_list.append(article)\n",
    "        else:\n",
    "            duplicates_count += 1\n",
    "    return new_list\n",
    "\n",
    "def connect_mongo(mongo_url, mongo_db):\n",
    "    client = MongoClient(mongo_url)\n",
    "    mongo_db_instance = client[mongo_db]\n",
    "    return mongo_db_instance\n",
    "\n",
    "def get_collection(mongo_db, collection_to_get):\n",
    "    collection = mongo_db[collection_to_get]\n",
    "    return collection\n",
    "\n",
    "def insert_all_in_collection(list_to_add, collection):\n",
    "    if len(list_to_add) > 0:\n",
    "        collection.insert_many(list_to_add)\n",
    "\n",
    "def check_article_exist(article_title):\n",
    "    articles = collection_article.find({'title': article_title})\n",
    "    for _ in articles:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def remove_duplicates_from_db(article_list):\n",
    "    new_list = []\n",
    "    for article in article_list:\n",
    "        exist = check_article_exist(article['title'])\n",
    "        if not exist:\n",
    "            new_list.append(article)\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_url = \"mongodb://\"\n",
    "mongo_db_name = \"kiev\"\n",
    "mongo_collection_article = \"articles\"\n",
    "synonyms_deaths = [\"death\", \"deaths\", \"murder\", \"killed\", \"kill\", \"died\", \"dying\", \"wounded\", \"wound\", \"missing\", \"casualties\"]\n",
    "\n",
    "mongo_db = connect_mongo(mongo_url, mongo_db_name)\n",
    "collection_article = get_collection(mongo_db, mongo_collection_article)\n",
    "\n",
    "articles_list = []\n",
    "for i in range(1, 100):\n",
    "    new_url = main_url + str(i)\n",
    "    response = requests.get(new_url, headers=headers, data=payload)\n",
    "    \n",
    "    try:\n",
    "        page_json = response.json()\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse JSON from {new_url}\")\n",
    "        break\n",
    "    \n",
    "    if page_json.get('message') == end_parsing_string:\n",
    "        break\n",
    "\n",
    "    print(f\"Processing: {new_url}\")\n",
    "    articles_list = convert_json_page_to_articles_list(page_json, articles_list)\n",
    "    \n",
    "    removed_duplicate_list_db = remove_duplicates_from_db(articles_list)\n",
    "    \n",
    "    if len(articles_list) != len(removed_duplicate_list_db):\n",
    "        print(\"Duplicates found in DB; stopping the fetch loop\")\n",
    "        articles_list = removed_duplicate_list_db\n",
    "        break\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "# Remove duplicates within the new articles list itself\n",
    "removed_duplicate_list = remove_duplicates(articles_list)\n",
    "\n",
    "# Insert only the new articles not already in DB\n",
    "insert_all_in_collection(removed_duplicate_list, collection_article)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92b7e0ce4c1cde783add4255e100d44ea3902933f047c43a85c7947d54610b37"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
